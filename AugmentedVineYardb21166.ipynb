{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ULQHjUQkNSs",
        "outputId": "466bd56c-b433-4483-9cdb-d46abd0020fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class VineyardDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        # Prepare list of files, checking if both image and mask exist\n",
        "        self.images = []\n",
        "        for image in os.listdir(images_dir):\n",
        "            if image.endswith('.png'):\n",
        "                img_path = os.path.join(images_dir, image)\n",
        "                mask_name = image.replace('.png', '_instanceIds.png')\n",
        "                mask_path = os.path.join(masks_dir, mask_name)\n",
        "                if os.path.exists(mask_path):  # Check if mask file exists\n",
        "                    self.images.append(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        mask_name = os.path.basename(img_path).replace('.png', '_instanceIds.png')\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "\n",
        "        #try:\n",
        "        #    image = Image.open(img_path).convert(\"RGB\")\n",
        "        #    mask = Image.open(mask_path).convert(\"L\")\n",
        "        #except FileNotFoundError:\n",
        "        #    return None  # Return None if there's an issue opening the file\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\")) # we are using np array since we will be using Albumentations library which req np array\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image = image, mask = mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "ydTpHICFdwqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# from dataset import VineyardDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import time\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    batch_size,\n",
        "    num_workers,\n",
        "    pin_memory = True\n",
        "):\n",
        "    train_data = VineyardDataset(images_dir=train_dir,\n",
        "                                masks_dir=train_maskdir,\n",
        "                                transform=train_transform)\n",
        "    val_data = VineyardDataset(images_dir=val_dir,\n",
        "                                masks_dir=val_maskdir,\n",
        "                                transform=val_transform)\n",
        "\n",
        "    train_dataloader = DataLoader(dataset=train_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=pin_memory)\n",
        "    val_dataloader = DataLoader(dataset=val_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=pin_memory)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def save_checkpoint(state, filename = \"/content/drive/MyDrive/VineNetData/saved_images/my_checkpoint.pth.tar\"):\n",
        "    print(\"==> Saving CheckPoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"==> Loading CheckPoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "# Define the evaluation metrics functions\n",
        "def calculate_precision(tp, fp):\n",
        "    if (tp+fp) == 0:\n",
        "      return 0.0\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "def calculate_recall(tp, fn):\n",
        "    if (tp+fn) == 0:\n",
        "      return 0.0\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "def calculate_mAP(tp, fp, fn):\n",
        "    precision = calculate_precision(tp, fp)\n",
        "    recall = calculate_recall(tp, fn)\n",
        "    if precision == 0 or recall == 0:\n",
        "        return 0.0\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "def measure_inference_time(model, data_loader):\n",
        "    model.eval()\n",
        "    total_time = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "            start_time = time.time()\n",
        "            _ = model(images)\n",
        "            end_time = time.time()\n",
        "            total_time += end_time - start_time\n",
        "    return total_time / len(data_loader)\n",
        "\n",
        "# Assuming test_loader is defined similar to train_loader\n",
        "def evaluate_model(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "            masks = masks.reshape(outputs.shape)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate true positives, false positives, and false negatives\n",
        "            predictions = (outputs > 0.5).float()\n",
        "            tp += torch.sum(predictions * masks).item()\n",
        "            fp += torch.sum(predictions * (1 - masks)).item()\n",
        "            fn += torch.sum((1 - predictions) * masks).item()\n",
        "\n",
        "    precision = calculate_precision(tp, fp)\n",
        "    recall = calculate_recall(tp, fn)\n",
        "    mAP = calculate_mAP(tp, fp, fn)\n",
        "    inference_time = measure_inference_time(model, val_loader)\n",
        "\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, mAP: {mAP:.4f}, Inference Time: {inference_time:.4f}, Loss: {total_loss / len(val_loader):.4f}')\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model, device):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(X))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2*(preds*y).sum())/((preds + y).sum()+ 1e-8)\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with accuracy {(num_correct/num_pixels)*100: .3f}\"\n",
        "    )\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def save_predictions_as_imgs(loader, model,device, folder_dir = \"saved_images/\"):\n",
        "    model.eval()\n",
        "\n",
        "    for idx, (X, y) in enumerate(loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.inference_mode():\n",
        "            preds = torch.sigmoid(model(X))\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "        torchvision.utils.save_image(preds, f\"{folder_dir}/pred_{idx}.png\")\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder_dir}{idx}.png\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "MasuSDlveha2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import math\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, dim_k):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.query = nn.Linear(dim_k, dim_k)\n",
        "#         self.key = nn.Linear(dim_k, dim_k)\n",
        "#         self.value = nn.Linear(dim_k, dim_k)\n",
        "#         self.dim_k = dim_k\n",
        "\n",
        "#     def forward(self, q, k, v, mask=None):\n",
        "#         # Apply linear transformations\n",
        "#         q = self.query(q)\n",
        "#         k = self.key(k)\n",
        "#         v = self.value(v)\n",
        "\n",
        "#         # Calculate the scale factor for the dot products\n",
        "#         scale = math.sqrt(self.dim_k)\n",
        "\n",
        "#         # Perform scaled dot-product attention\n",
        "#         attention_scores = torch.matmul(q, k.transpose(-1, -2)) / scale\n",
        "\n",
        "#         # Optional: Apply mask here if not None\n",
        "\n",
        "#         # Apply softmax to get probabilities\n",
        "#         attention = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "#         # Multiply by values\n",
        "#         output = torch.matmul(attention, v)\n",
        "\n",
        "#         return output\n",
        "\n",
        "# class BottleNeck(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, dim_k):\n",
        "#         super(BottleNeck, self).__init__()\n",
        "#         self.attention = Attention(dim_k)\n",
        "#         self.down_layer = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dim_k = dim_k  # Ensure this is used or removed appropriately\n",
        "\n",
        "#     def forward(self, X):\n",
        "#         out = self.down_layer(X)\n",
        "\n",
        "#         # Reshape for attention. Ensure dimensions are correctly aligned\n",
        "#         B, C, H, W = out.shape\n",
        "#         q = out.view(B, C, H * W).transpose(1, 2)  # Reshape to (B, H*W, C)\n",
        "#         k = q  # Same shape for simplicity in this example\n",
        "#         v = q  # Same shape for simplicity in this example\n",
        "\n",
        "\n",
        "#         attention_output = self.attention(q, k, v, None)\n",
        "\n",
        "#         # Reshape attention output to match input feature map dimensions\n",
        "#         attention_output = attention_output.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "#         # Combine the attention output with the input feature map\n",
        "#         out = out + attention_output\n",
        "\n",
        "#         return out\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.down_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, X):\n",
        "        return self.down_layer(X)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Up, self).__init__()\n",
        "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2) # 1024 --> 512\n",
        "        self.DoubleConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, skip_connection):\n",
        "        X1 = self.up_conv(X)\n",
        "        if(X1.shape != skip_connection.shape):\n",
        "            X1 = TF.resize(X1, skip_connection.shape[2:])\n",
        "        X2 = torch.cat((X1, skip_connection), dim=1)\n",
        "\n",
        "        return self.DoubleConv(X2)\n",
        "\n",
        "class FinalConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(FinalConv, self).__init__()\n",
        "        self.finalConv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.finalConv(X)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels = 3, out_channels = 1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.down1 = Down(in_channels, 64)\n",
        "        self.down2 = Down(64, 128)\n",
        "        self.down3 = Down(128, 256)\n",
        "        self.down4 = Down(256, 512)\n",
        "\n",
        "        # self.bottleNeck = Down(512, 1024)\n",
        "        self.bottleNeck = Down(512, 1024)\n",
        "\n",
        "\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "\n",
        "        self.finalConv = FinalConv(64, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        ### DownSampling\n",
        "        x1_skip = self.down1(X)\n",
        "        x1 = self.max_pool(x1_skip)\n",
        "\n",
        "        x2_skip = self.down2(x1)\n",
        "        x2 = self.max_pool(x2_skip)\n",
        "\n",
        "        x3_skip = self.down3(x2)\n",
        "        x3 = self.max_pool(x3_skip)\n",
        "\n",
        "        x4_skip = self.down4(x3)\n",
        "        x4 = self.max_pool(x4_skip)\n",
        "\n",
        "        ### BottleNeck Layer\n",
        "        x5 = self.bottleNeck(x4)\n",
        "\n",
        "        ### UpSampling\n",
        "        x  = self.up1(x5, x4_skip)\n",
        "        x  = self.up2(x , x3_skip)\n",
        "        x  = self.up3(x , x2_skip)\n",
        "        x  = self.up4(x , x1_skip)\n",
        "        x  = self.finalConv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# def test():\n",
        "#     x = torch.randn(3, 1, 572, 572)\n",
        "#     model = UNet(in_channels=1, out_channels=1)\n",
        "#     preds = model(x)\n",
        "#     print(f\"Preds shape: {preds.shape}\")\n",
        "#     print(x.shape == preds.shape)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     test()"
      ],
      "metadata": {
        "id": "in05cuPpeqhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "# from u_net_model import UNet\n",
        "from torch.utils.data import DataLoader\n",
        "# from dataset import VineyardDataset\n",
        "\n",
        "# from utils import(\n",
        "#     load_checkpoint,\n",
        "#     save_checkpoint,\n",
        "#     get_loaders,\n",
        "#     check_accuracy,\n",
        "#     save_predictions_as_imgs\n",
        "# )\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_WIDTH = 256\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/VineNetData/train/images\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/VineNetData/train/masks\"\n",
        "VAL_IMG_DIR = \"/content/drive/MyDrive/VineNetData/val/images\"\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/VineNetData/val/masks\"\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    train_losses = []\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        model.to(device=DEVICE)\n",
        "        data = data.to(device = DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device = DEVICE)\n",
        "\n",
        "        # forward\n",
        "        model.train()\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        print((loss.to(\"cpu\")).detach().numpy())\n",
        "        train_losses.append(loss.to(\"cpu\").detach().numpy())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        return train_losses\n",
        "\n",
        "### TRAINING\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Rotate(limit=35, p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std = [1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std = [1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "new_model = UNet(in_channels=3, out_channels=1).to(device=DEVICE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(params=new_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loader, val_loader = get_loaders(train_dir=TRAIN_IMG_DIR,\n",
        "                                        train_maskdir=TRAIN_MASK_DIR,\n",
        "                                        val_dir=VAL_IMG_DIR,\n",
        "                                        val_maskdir=VAL_MASK_DIR,\n",
        "                                        train_transform=train_transform,\n",
        "                                        val_transform=val_transform,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        num_workers=NUM_WORKERS,\n",
        "                                        pin_memory=PIN_MEMORY)\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), new_model)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "train_losses = []\n",
        "# loop = tqdm(train_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"------Epoch: {epoch}------\")\n",
        "\n",
        "    # train_losses = train_fn(train_loader, new_model, optimizer, loss_fn, scaler)\n",
        "    loop = tqdm(train_loader)\n",
        "\n",
        "    train_loss = []\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        new_model.to(device=DEVICE)\n",
        "        data = data.to(device = DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device = DEVICE)\n",
        "\n",
        "        # forward\n",
        "        new_model.train()\n",
        "        predictions = new_model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        train_loss.append(loss.to(\"cpu\").detach().numpy())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "    train_losses.append(sum(train_loss)/len(train_loss))\n",
        "    # if(len(train_losses)>3):\n",
        "    #   if(abs(train_losses[-1]-train_losses[-2])<0.0001):\n",
        "    #     checkpoint = {\"state_dict\": new_model.state_dict(),\"optimizer\": optimizer.state_dict()}\n",
        "    #     save_checkpoint(checkpoint)\n",
        "    #     break\n",
        "\n",
        "    # save the new_model\n",
        "    checkpoint = {\n",
        "        \"state_dict\": new_model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict()\n",
        "    }\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    check_accuracy(val_loader, new_model, device= DEVICE)\n",
        "    # evaluate_new_model(new_model, loss_fn, val_loader)\n",
        "\n",
        "    # print some examples to a folder\n",
        "    save_predictions_as_imgs(\n",
        "        val_loader,\n",
        "        new_model,\n",
        "        folder_dir=\"/content/drive/MyDrive/VineNetData/saved_images\",\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "# def test_fn():\n",
        "#     pass\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "DkWfFEeFethK",
        "outputId": "58ecb959-e00f-407d-b771-c5b278c652b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Epoch: 0------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/36 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 36/36 [07:30<00:00, 12.51s/it, loss=0.225]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Saving CheckPoint\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c5d53104ba65>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;31m# evaluate_new_model(new_model, loss_fn, val_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-589d38907a2c>\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(loader, model, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot losses versus epochs\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linestyle='-')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs. Epoch')\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sjPX5Gx0lyNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for images, masks in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()\n",
        "            outputs = model(images)\n",
        "            print(outputs.shape)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "    print(f'Validation Loss: {total_loss / len(val_loader):.4f}')\n",
        "\n",
        "    model.train()  # Set model back to training mode\n"
      ],
      "metadata": {
        "id": "aBIzH1naewVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Define the evaluation metrics functions\n",
        "def calculate_precision(tp, fp):\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "def calculate_recall(tp, fn):\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "def calculate_mAP(tp, fp, fn):\n",
        "    precision = calculate_precision(tp, fp)\n",
        "    recall = calculate_recall(tp, fn)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "#------------------------------------\n",
        "\n",
        "def compute_iou_and_miou(outputs, labels, num_classes):\n",
        "    intersection = torch.zeros(num_classes)\n",
        "    union = torch.zeros(num_classes)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        intersection[cls] = torch.logical_and(outputs == cls, labels == cls).sum()\n",
        "        union[cls] = torch.logical_or(outputs == cls, labels == cls).sum()\n",
        "\n",
        "    iou = torch.div(intersection, union)\n",
        "    valid_classes = union.nonzero().size(0)\n",
        "    miou = iou.sum() / valid_classes\n",
        "\n",
        "    return iou, miou\n",
        "\n",
        "#----------------------------------------\n",
        "\n",
        "\n",
        "def jaccard_index(gt, pred):\n",
        "    pred = pred.to(\"cpu\").numpy()\n",
        "    gt = gt.to(\"cpu\").numpy()\n",
        "    pred = np.where(pred >= 0.5, 1, 0)\n",
        "    gt = (gt >= 0.5).astype(int)\n",
        "    intersection = np.sum(gt * pred)\n",
        "    union = np.sum(gt) + np.sum(pred) - intersection\n",
        "    jaccard = intersection / (union + 1e-8)\n",
        "    return jaccard\n",
        "\n",
        "def measure_inference_time(model, data_loader):\n",
        "    model.eval()\n",
        "    total_time = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "            start_time = time.time()\n",
        "            _ = model(images)\n",
        "            end_time = time.time()\n",
        "            total_time += end_time - start_time\n",
        "    return total_time / len(data_loader)\n",
        "\n",
        "# Assuming test_loader is defined similar to train_loader\n",
        "def evaluate_model(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "            masks = masks.unsqueeze(1)\n",
        "            # print(masks.shape)\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate true positives, false positives, and false negatives\n",
        "            predictions = (outputs > 0.5).float()\n",
        "            tp += torch.sum(predictions * masks).item()\n",
        "            fp += torch.sum(predictions * (1 - masks)).item()\n",
        "            fn += torch.sum((1 - predictions) * masks).item()\n",
        "\n",
        "    precision = calculate_precision(tp, fp)\n",
        "    recall = calculate_recall(tp, fn)\n",
        "    mAP = calculate_mAP(tp, fp, fn)\n",
        "    inference_time = measure_inference_time(model, val_loader)\n",
        "    iou, miou = compute_iou_and_miou(predictions, masks, 1)\n",
        "    jacc_list = []\n",
        "    for i in range(len(masks)):\n",
        "      jacc_list.append((jaccard_index(masks[i], predictions[i])))\n",
        "    jacc_list = np.array(jacc_list)\n",
        "\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, mAP: {mAP:.4f}, miou: {miou}, Jaccard_index: {np.mean(jacc_list)}, Inference Time: {inference_time:.4f}, Loss: {total_loss / len(val_loader):.4f}')\n",
        "\n",
        "evaluate_model(new_model, loss_fn, val_loader)"
      ],
      "metadata": {
        "id": "tONAEeqoe1yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def visualize_test_results(images, masks, outputs, num_images=3):\n",
        "    fig, axs = plt.subplots(nrows=3, ncols=num_images, figsize=(10, 10), sharex=True, sharey=True)\n",
        "    for i in range(num_images):\n",
        "        axs[0, i].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
        "        axs[0, i].set_title('Input Image')\n",
        "        axs[0, i].axis('off')\n",
        "\n",
        "        # print(masks.shape)\n",
        "        axs[1, i].imshow(masks[i].cpu(), cmap='gray')\n",
        "        axs[1, i].set_title('True Mask')\n",
        "        axs[1, i].axis('off')\n",
        "\n",
        "        axs[2, i].imshow(outputs[i][0].sigmoid().detach().cpu().numpy() > 0.5, cmap='gray')\n",
        "        axs[2, i].set_title('Predicted Mask')\n",
        "        axs[2, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming test_loader is set up similarly to train_loader\n",
        "new_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, masks in val_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "        outputs = new_model(images)\n",
        "        visualize_test_results(images, masks, outputs)\n",
        "        break  # Only display one batch\n"
      ],
      "metadata": {
        "id": "qY275g9DezHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a trained model and a test dataset (test_loader)\n",
        "new_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming you have a test dataset named 'test_dataset'\n",
        "# and batch_size is set to 10\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, images_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.image_paths = [os.path.join(image_dir, img_name) for img_name in os.listdir(images_dir) if img_name.endswith('.png')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "image_dir = '/content/drive/MyDrive/vinenet_testdata/images'\n",
        "test_dataset = TestDataset(image_dir, transform=None)\n",
        "\n",
        "# Define the test loader\n",
        "# test1_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "def custom_collate(batch):\n",
        "    # Convert PIL.Image.Image objects to tensors\n",
        "    transformed_batch = [ToTensor()(item) for item in batch]\n",
        "    return torch.utils.data.dataloader.default_collate(transformed_batch)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming test1_loader is your DataLoader\n",
        "test1_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, collate_fn=custom_collate)\n",
        "\n",
        "#----------------------------\n",
        "# Example:\n",
        "new_model = UNet(3, 1)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "checkpoint_path = \"/content/drive/MyDrive/VineNetData/saved_images/my_checkpoint.pth.tar\"\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state dictionary into your model\n",
        "new_model.load_state_dict(checkpoint['state_dict'])\n",
        "new_model.to(\"cuda\")\n",
        "\n",
        "# Ensure that the model is in evaluation mode\n",
        "new_model.eval()\n",
        "\n",
        "#----------------------------\n",
        "# Create an empty list to store the predicted masks\n",
        "predicted_masks = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images in test1_loader:  # Iterate over the test dataset\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = new_model(images)\n",
        "\n",
        "        # Convert the output to probabilities by applying the sigmoid function\n",
        "        # You might need to adjust this depending on your model architecture\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) based on a threshold\n",
        "        # You can adjust the threshold based on your model and dataset\n",
        "        binary_predictions = (probabilities > 0.5).float()\n",
        "\n",
        "        # Append the predicted masks to the list\n",
        "        predicted_masks.append(binary_predictions.cpu())  # Convert to CPU if necessary\n",
        "\n",
        "# Concatenate the predicted masks along the batch dimension\n",
        "# This assumes that your test_loader batches contain only one image each\n",
        "predicted_masks = torch.cat(predicted_masks, dim=0)\n",
        "\n",
        "# predicted_masks\n"
      ],
      "metadata": {
        "id": "wlYZ56N6AMao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Assuming you want to save the masks to a directory named 'predicted_masks'\n",
        "output_dir = '/content/drive/MyDrive/vinenet_testdata/masks'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save each predicted mask as an image with the same filename as the corresponding test image\n",
        "for i, (image, mask) in enumerate(zip(test_dataset, predicted_masks)):\n",
        "    # Get the filename of the corresponding test image\n",
        "    test_filename = os.path.basename(test_dataset.image_paths[i])\n",
        "\n",
        "    # Save the predicted mask image with the same filename as the test image\n",
        "    mask_image = to_pil_image(mask.byte()).resize(1920, 960)\n",
        "    mask_image.save(os.path.join(output_dir, test_filename))\n",
        "\n",
        "    # Visualize and save the masks (optional)\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Original image\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.imshow(image.permute(1, 2, 0))\n",
        "#     plt.title('Original Image')\n",
        "#     plt.axis('off')\n",
        "\n",
        "#     # Predicted mask\n",
        "#     plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask.squeeze(), cmap='gray')\n",
        "#     plt.title('Predicted Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(os.path.join(output_dir, test_filename))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "YlH64H3gAQrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# specifying the zip file name\n",
        "directory = \"/content/drive/MyDrive/vinenet_testdata/masks\"\n",
        "file_name = \"P05_B_predictedimages.zip\"\n",
        "\n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'w') as zip_file:\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zip_file.write(file_path, os.path.relpath(file_path, os.path.join(directory, '..')))\n",
        "\n",
        "print(f\"Done\")"
      ],
      "metadata": {
        "id": "pdyB3oFaAT1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Path to the test data folder\n",
        "test_data_folder = \"/content/drive/MyDrive/vinenet_testdata/images\"\n",
        "\n",
        "# Create a folder to save the segmentation masks\n",
        "output_folder = \"/content/drive/MyDrive/vinenet_testdata/masks\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Function to perform segmentation and save masks\n",
        "def generate_masks(image_path, output_folder):\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    mask = new_model(image)\n",
        "    mask = mask.resize(1920, 960)\n",
        "\n",
        "    # Assuming a simple thresholding example\n",
        "    # gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # _, binary_mask = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Save the mask\n",
        "    mask_filename = os.path.basename(image_path).split('.')[0] + '_mask.png'\n",
        "    mask_path = os.path.join(output_folder, mask_filename)\n",
        "    cv2.imwrite(mask_path, binary_mask)\n",
        "\n",
        "# Iterate over each image in the test data folder\n",
        "for filename in os.listdir(test_data_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        image_path = os.path.join(test_data_folder, filename)\n",
        "        generate_masks(image_path, output_folder)\n"
      ],
      "metadata": {
        "id": "v_uj74cwnLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWStNHJr-4r2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}